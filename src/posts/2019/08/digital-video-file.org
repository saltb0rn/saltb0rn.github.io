#+title: 视频文件简单了解
#+date: 2019-08-18
#+index: 视频文件简单了解
#+tags: Video-file

#+macro: frame 帧
#+macro: interlace-frame 交错帧
#+macro: progressive-frame 渐近帧
#+macro: format 文件格式
#+macro: codec 编码解码器
#+macro: filter 滤镜
#+macro: bitrate 比特率

#+begin_abstract
因为临时的工作需要,要处理视频无法在 =ckplayer= 上面播放的问题,久违地使用了一下 =ffmpeg=.

在17年的时候就买了 =ffmpeg= 官网上推荐的 =ffmpeg basics= 来学习 =ffmpeg=,那个时候也是因为工作需求,

但是那个时候我还没有开始写 =blog=,所以这段时间会重新学习一边,顺便写些东西.

(从7.25日开始忙着下班玩火纹:风花雪月,已经偷懒了一段时间了,看来要把学习和锻炼像吃饭睡觉一样作为日常生活必选项).

复习 =ffmpeg= 之前先了解以下视频文件的一些基本概念,否则读 =ffmpeg basics= 会懵,我觉得这本书唯一的缺点就是没有先介绍视频文件,

对 =codec= ({{{codec}}}), =format= ({{{format}}})等基础概念的不理解会造成对 =ffmpeg= 的工作流程的不理解.
#+end_abstract

在深入了解视频一些技术参数之前先讲一下视频文件的相关知识.

*** 视频编码和解码

通常录制完的视频都需要做压缩处理的,否则文件大小会十分庞大不方便传输和储存.压缩也就是视频编码,编码的组件叫做 =encoder=,

对应压缩就有解压,也就是视频解码,解码的组件叫做 =decoder=,两个组件组合起来就是一个叫做{{{codec}}}的软件/硬件.

值得一提的是压缩(基本都)是失真的,也就是说后面对视频进行解码也是不能得到和原来一样的视频.

而视频转码的概念也就是换种编码方式:先用旧的编码方式解码,然后用另外一种方式给解码后的数据进行编码.


*** 视频{{{format}}}以及视频文件构成

一个视频文件是由视频数据和音频数据组成的,视频文件数据实际上要放到一个容器 =F= 里面,

而容器 =F= 又是储存了用于存放视频数据的子容器 =V= 和用于存放音频数据的子容器 =A= ,容器 =F= 还可能包含视频同步信息,视频文件元信息(=metadata=)以及字幕.

同样,子容器 =V= 和 =A= 也有自己对应的元信息,这些元信息是储存了对应容器内容的一些相关信息.容器 =F= 就是我们说的视频{{{format}}}, =mp4=, =mkv= 等等就是常见的视频文件容器;

子容器 =V= 就是视频编码格式/视频压缩格式(=video coding format/video compression format=),比如 =H.264=, =MPEG4-2= 等等,比如对应 =H.264= 的 =codec= 叫做 =H264=.

子容器 =A= 就是音频编码格式/音频压缩格式(=audio coding format/audio compression format=),比如大名鼎鼎的 =mp3=.

因为由两个数据,所以播放的时候需要同步视频和音频,这个时候视频文件的同步信息就发挥作用了.

一种的视频{{{format}}}由特定的视频编码格式以及音频编码格式组成,比如 =mp4= 可以由 =H.264= 以及 =mp3= 两种格式组成,也可以是由 =MPEG-1= 和 =MP3= 两种格式组成;此外,不同的视频文件格式里面的内容排列是也会由一些差别.

比如只有 =MP4= 和 =QuickTime= 文件的 =box= 储存形式,有兴趣可以看 =MP4= 的[[https://www.cnwrecovery.com/manual/MP4FileLayouts.html][文件布局]].各种常见的格式可以看看这[[https://en.m.wikipedia.org/wiki/Video_file_format][维基页]].

正如上个小节提到的,视频是要先经过编码后再以某种格式把视频数据和音频数据封装成一个文件进行储存的,负责封装的组件叫做 =muxer=,同样对应封装的还有分离/分流,负责这块的逐渐叫做 =demuxer=.

到这里为止,可以自行去查找关于视频播放的原理了.


*** 视频数据之{{{frame}}}

    视频数据实际上就是一系列的连续图片,也就是动画实现的原理,每一张图片叫做{{{frame}}}(=frame=).

    常说的一个视频每秒XX{{{frame}}},也就是指这个视频的{{{frame}}}率为 =XX fps= 或者写作 =XX f/s=.

    =fps= 或者 =f/s= 的缩写是 =frames per second=.

    比如一个视频30fps,那么就是这个视频一秒播放30张图片,所以在图片质量不变的情况下改变{{{frame}}}率可以改变视频大小.

    一{{{frame}}}就是一张位图(=bitmap=),位图是一个亮度和像素数量两纬数组表示的,而像素只有一个属性,就是颜色.

    一个像素的颜色是由固定数量的位(=bits=)表示的,这个数量叫做色彩深度(=color depth=),位越多,可用的颜色越多,24位色彩深度就有 *2的24次方* 种颜色可用.

    {{{frame}}}的颜色深度也就是视频的颜色深度.

    由于{{{frame}}}是一张位图,那么也就是说图片的像素大小等于 =(height * weight)=,储存大小就是 (=像素大小 * 2的色彩深度次方=).

    {{{frame}}}有两种: {{{interlace-frame}}}(=interlace frame=) 以及{{{progressive-frame}}}(=progressive frame=).


**** {{{interlace-frame}}}

    采用{{{interlace-frame}}}的视频叫做交错视频(=interlaced video=),每一完整{{{frame}}}都是由一张图片的两半组成的,

    第一半包含了图片的奇数行(=odd-numbered lines=),第二半就包含了图片的偶数行(=even-numbered lines=).

    这样的一半叫做域(=filed=),也叫做{{{interlace-frame}}},两个连续的域构成一个完整的{{{frame}}}.一个30fps交错视频每秒播放60个域.

    每秒60域可以写作 60 =fields per second=,缩写是 =60i fps=,而不是 =60fps=,注意 =i=, =i= 表示交错.


**** {{{progressive-frame}}}


     和{{{interlace-frame}}}不同的是,{{{progressive-frame}}}是逐行/连续扫瞄完一张图片的.

     平常说的一个视频的每秒XX{{{frame}}}就是指的每秒XX{{{progressive-frame}}},所以 =30fps= 也就是 =30p fps=.


**** {{{interlace-frame}}}与{{{progressive-frame}}}

     两者的关系就是: =60i fps = 30p fps= 或者说 =60i fps = 30fps=.交错视频和非交错视频是可以相互转换的.

     大部份视频设备都是交错的,这是为了减少传输带来的带宽压力以及阻止闪烁,而大部份视频都是非交错视频,所以转换是必须的.

     正如上面{{{interlace-frame}}}章节中说到的,一个完整会被隔行分成两半,同样,也可以把非交错视频的每个{{{frame}}}按照交错视频那样分成两半.

     这样分出来的一半叫做渐近段{{{frame}}}(=progressive segmented frame=,缩写 =PsF/sF/SF=),技术上来说,{{{interlace-frame}}}和渐近段{{{frame}}}是一样的.

     与原生交错视频不同的是,两个段{{{frame}}}之间是没有动作补间(=motion=)的.


**** Filter

     对视频数据进行处理实际上就是对{{{frame}}}进行处理,处理{{{frame}}}的这个组件叫做 =filter=,因为视频是连续画面,所以处理每一{{{frame}}}就相当于在过滤{{{frame}}}.

     =filter= 有不同的种类,有着不同的能力,过滤{{{frame}}}不一定要修改{{{frame}}}本身,同样,音频数据也是有 =filter= 的.总而言之,如果要掌握处理视频的技巧, 那么熟悉各种 =filter= 是必须的.


*** 视频数据之{{{bitrate}}}以及视频文件大小

    {{{bitrate}}},或者说码率,说明了每个时间单元内处理多少 =bits=,这个时间单元通常就是一秒,这个参数决定了视频数据的质量以及大小.

    同样,音频数据也是有{{{bitrate}}}的,也是决定了音频的质量以及大小,单位都是 =bits/s= 或者 =bps=

    从另外一个来看,视频数据的{{{bitrate}}}就是每秒XX{{{frame}}}的大小.

    比如说一个视频的{{{frame}}}率是 =60fps=,视频数据的{{{bitrate}}}是 =1500kbps=,那么平均一{{{frame}}}大小就是 =1500kbps / 60fps=,如何判断视频质量的好坏,转码的时候如何选择{{{bitrate}}}?

    假如一个视频{{{frame}}}的宽高为 =1920 * 1080=,那么一秒就是 =60 * 1920 * 1080= 个像素点,那么每个像素点占用的 =bits= 为 =(1500 * 1024 * 8) / (60 * 1920 * 1080)=,大约为0.098.

    这个值叫做 =Bits per pixel=,缩写为 =bpp=,如果这个值在 =0.1= 就说明有一个很好的视频质量了,高于这个值不会产生视觉上能感受到的提高,在 =0.03= 附近就表示质量已经很低,再低就不能看了.


    {{{bitrate}}}有三种:平均{{{bitrate}}} =(Average bit rate)=,固定{{{bitrate}}} =(Constant bit rate)= 以及可变{{{bitrate}}} =(Variable bit rate)=.

    缩写分别是 =abr, cbr 和 vbr=. =abr= 和 =cbr= 可以从它们的名字就看出不同,前者是每一秒的 =bits= 数量不一样,后者是每秒固定数量 =bits=.

    =cbr= 有个缺点就是不利于储存,因为不同复杂读的画面场景或者声音会需要不同空间,比起静态场面,快速运动场面需要更多空间,那么就有不必要的空间浪费, =cbr= 的优点是方便传输.

    而 =abr= 是用于产出特定大小的视频文件的.至于 =vbr= 同样是每秒不同 =bits= 数量,它是根据复杂度来分配的,相对于 =cbr= 它在复杂情况下需要更多储存空间,

    在同样的文件大小下 =vbr= 比 =cbr= 的质量更好,由于 =vbr= 是按需求分配 =bits=,所以 =vbr= 编码时使用 =vbr= 会比使用 =cbr= 更加消耗 =CPU= 时间.


    下面就是视频文件大小的计算公式:

    *视频文件大小 = 视频数据大小 + 音频数据大小 + 一些其他信息大小*; (这里先不考虑其他信息).

    *视频数据大小 = 视频数据{{{bitrate}}} X 播放时间长度*;

    音频的计算要分两种情况,未压缩以及己经压缩,

    在未压缩的情况下:

    *音频数据大小 = 音频采样率(sampling rate) X 音频位深度(bit depth) X 音频通道数(channels) X 播放时间长度*

    在已压缩的情况下:

    *音频数据大小 = 音频数据{{{bitrate}}} X 播放时间长度*

    通常音频数据播放长度和视频数据的是一样的,一般来说音频数据也是经过压缩的,所以经过化简后:

    *视频文件大小 = (视频数据{{{bitrate}}} + 音频数据{{{bitrate}}}) * 播放时间长度*.

    用 =ffprobe= 可以查看到视频文件的总{{{bitrate}}}以及视频数据和音频数据分别的{{{bitrate}}}.


*** 视频解析度

    视频解析度就是每一{{{frame}}}的宽度和高度,比如 =1920 * 1080=,也就是常说的 =1080p=,这要满足一个条件,

    视频的宽高比一定要是 =16:9=, =1080= 表示视频的垂直高度, =p= 表示渐进{{{frame}}},也就是逐行扫描.

    如此类推, =720p= 就表示 =宽 * 高= 为 =1280 * 720= 的视频.有时候会在后面加上{{{frame}}}率,比如 =720p 30fps=.
